{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [\"FP1-F7\", \"F7-T7\", \"T7-P7\", \"P7-O1\", \"FP1-F3\", \"F3-C3\", \n",
    "            \"C3-P3\", \"P3-O1\", \"FP2-F4\", \"F4-C4\", \"C4-P4\", \"P4-O2\", \n",
    "            \"FP2-F8\", \"F8-T8\", \"P8-O2\", \"FZ-CZ\", \"CZ-PZ\"]\n",
    "\n",
    "number_of_channels = len(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = [2, 3, 4, 5, 6, 7, 10, 15, 30, 50, 100]\n",
    "classes_duration = 60 * 60\n",
    "length = 60\n",
    "interval = 300 * 60\n",
    "number_of_classes = interval // classes_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['1', '5']\n",
    "classes_to_classify = [['1', '5'],\n",
    "                       ['1', '2', '3', '4', '5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustring(B, cluster):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster, random_state=0).fit(B)\n",
    "    centeroids = kmeans.cluster_centers_\n",
    "    \n",
    "    return centeroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(a, b):\n",
    "    \n",
    "    x1, x2 = a[0], b[0]\n",
    "    y1, y2 = a[1], b[1]\n",
    "    X, Y = x2-x1, y2-y1\n",
    "    \n",
    "    return np.sqrt(X**2 + Y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ctc in classes_to_classify:\n",
    "    os.makedirs(f\"Data/Interval-300-classes-length-{classes_duration//60}/Features/classification-{'-'.join(ctc)}\")\n",
    "    for cluster in number_of_clusters:\n",
    "        os.makedirs(f\"Data/Interval-300-classes-length-{classes_duration//60}/Features/classification-{'-'.join(ctc)}/{cluster}-clusters\")\n",
    "        for i in ctc:\n",
    "            os.makedirs(f\"Data/Interval-300-classes-length-{classes_duration//60}/Features/classification-{'-'.join(ctc)}/{cluster}-clusters/class-{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints_after_pca_path = f\"Data/Interval-300-classes-length-{classes_duration//60}/data-points-after-pca\"\n",
    "combined_channels_data_path = f\"Data/Interval-300-classes-length-{classes_duration//60}/combined-channels-data\"\n",
    "cluster_centroids_path = f\"Data/Interval-300-classes-length-{classes_duration//60}/cluster_centroids\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering And Feature Extraction\n",
    "\n",
    "## Phase 1 (Performing clustering on each of the classes) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 30, 50, 100]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_clusters[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.load(f\"{combined_channels_data_path}/class-{1}/channel_{0}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(number_of_channels):\n",
    "#     channel_data = np.load(f\"{combined_channels_data_path}/class-{1}/channel_{i}.npy\")\n",
    "#     d = clustring(channel_data, 50)\n",
    "#     del channel_data\n",
    "#     print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['1', '2', '3', '4', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "647.7032568454742\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-54426a4974bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mchannel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{combined_channels_data_path}/class-{_class}/channel_{i}.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mcenteroids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mchannel_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-af3a14ccd953>\u001b[0m in \u001b[0;36mclustring\u001b[0;34m(B, cluster)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclustring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniBatchKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcenteroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcenteroids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minertia_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels_inertia_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_labels_inertia_minibatch\u001b[0;34m(self, X, sample_weight)\u001b[0m\n\u001b[1;32m   1808\u001b[0m         \u001b[0mx_squared_norms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m         \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m         results = [_labels_inertia(X[s], sample_weight[s], x_squared_norms[s],\n\u001b[0m\u001b[1;32m   1811\u001b[0m                                    self.cluster_centers_) for s in slices]\n\u001b[1;32m   1812\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1808\u001b[0m         \u001b[0mx_squared_norms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m         \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m         results = [_labels_inertia(X[s], sample_weight[s], x_squared_norms[s],\n\u001b[0m\u001b[1;32m   1811\u001b[0m                                    self.cluster_centers_) for s in slices]\n\u001b[1;32m   1812\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_labels_inertia\u001b[0;34m(X, sample_weight, x_squared_norms, centers, n_threads)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0m_inertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inertia_dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     _labels(X, sample_weight, x_squared_norms, centers, centers,\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0mweight_in_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter_shift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             update_centers=False)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for cluster in number_of_clusters[7:]:    \n",
    "    for _class in classes[1:-1]:\n",
    "        centeroids = []\n",
    "        t1 = time.time()\n",
    "\n",
    "        for i in range(number_of_channels):\n",
    "            channel_data = np.load(f\"{combined_channels_data_path}/class-{_class}/channel_{i}.npy\")\n",
    "\n",
    "            centeroids.append(clustring(channel_data, cluster))\n",
    "            del channel_data\n",
    "            print('yes')\n",
    "\n",
    "        np.save(f\"{cluster_centroids_path}/class-{_class}/{cluster}_clusters_centeroids.npy\", centeroids)\n",
    "        t2 = time.time()\n",
    "        print(t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 (Declaring number of datapoints in each cluster) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n"
     ]
    }
   ],
   "source": [
    "classes_to_classify = [['1', '5']]\n",
    "for ctc in classes_to_classify:\n",
    "    print(ctc[0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 3]\n",
    "a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Interval-300-classes-length-60/Features/classification-1-5\n"
     ]
    }
   ],
   "source": [
    "for ctc in classes_to_classify:\n",
    "    features_path = f\"Data/Interval-300-classes-length-{length}/Features/classification-{'-'.join(ctc)}\"\n",
    "    print(features_path)\n",
    "    \n",
    "    for cluster in number_of_clusters[10:11]:    \n",
    "        # Fist load all of this cluster centroids in order to do things faster\n",
    "        centroids = []\n",
    "        for _class in ctc:\n",
    "            centroids.append(np.load(f\"{cluster_centroids_path}/class-{_class}/{cluster}_clusters_centeroids.npy\"))\n",
    "\n",
    "        for class1 in ctc[1:]:\n",
    "            t1 = time.time()\n",
    "            current_class_path = f\"{datapoints_after_pca_path}/class-{class1}\"\n",
    "            current_class_list = os.listdir(current_class_path)\n",
    "\n",
    "            for file in current_class_list[:11000]:\n",
    "                current_class_data = np.load(current_class_path + '/' + file)\n",
    "\n",
    "                features = []\n",
    "                for i in range(number_of_channels):\n",
    "                    channel_number_in_cluster = list(np.zeros((cluster * len(ctc)), dtype=int))\n",
    "\n",
    "                    for datapoint in current_class_data[i]:\n",
    "                        index_jump = 0\n",
    "\n",
    "                        for count in range(len(ctc)): \n",
    "                            # Calculate number of current class datapoints in all classes clusters\n",
    "                            distances = [distance(datapoint, centeroid) for centeroid in centroids[count][i]]\n",
    "                            channel_number_in_cluster[distances.index(min(distances)) + index_jump] += 1\n",
    "                            index_jump += cluster\n",
    "\n",
    "                    features.append(channel_number_in_cluster)\n",
    "\n",
    "                # Saving features\n",
    "                np.save(f\"{features_path}/{cluster}-clusters/class-{class1}/{file}\", features)\n",
    "\n",
    "\n",
    "            t2 = time.time()\n",
    "            print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"shutdown /s /t 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('systemctl poweroff') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
